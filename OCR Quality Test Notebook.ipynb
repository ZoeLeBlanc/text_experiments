{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import ldamodel\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import string\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from collections import OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.readwrite import json_graph\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.layouts import row, column\n",
    "import random\n",
    "\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATA for each case\n",
    "full_df=pd.read_csv('full_page_image_lucida_test.csv')\n",
    "split_df = pd.read_csv('split_page_image_lucida_test.csv', encoding='utf-8')\n",
    "# order_df.isnull().values.any()\n",
    "order_df = pd.read_csv('ordered_text_image_lucida_test.csv', encoding=\"ISO-8859-1\")\n",
    "order_df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Co-Occurence Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    if not text:\n",
    "#       print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def process_text(df, types, graph_settings):\n",
    "    doc = []\n",
    "    final_doc = []\n",
    "    for index, row in df.iterrows():\n",
    "        raw_text = row['google_vision_text']\n",
    "        tokens = custom_tokenize(raw_text)\n",
    "        page_terms = ''\n",
    "        for t in tokens:\n",
    "            if t in string.punctuation:\n",
    "                pass\n",
    "            elif t in stopwords.words('english'):\n",
    "                pass\n",
    "            else:\n",
    "                page_terms += t.lower() + ' '\n",
    "        doc.append(page_terms)\n",
    "\n",
    "    for sent in doc:\n",
    "        sent_terms = ''\n",
    "        spacy_text = nlp(sent)\n",
    "        for ent in spacy_text.ents:\n",
    "            if ent.label_ in types:\n",
    "                sent_terms += ent.text + ' '\n",
    "        final_doc.append(sent_terms)\n",
    "    return create_matrix(final_doc, graph_settings)\n",
    "    \n",
    "def create_matrix(ents, graph_settings):\n",
    "    count_model = CountVectorizer(ngram_range=(1,1)) # default unigram model\n",
    "    X = count_model.fit_transform(ents)\n",
    "    Xc = (X.T * X)\n",
    "    vocab = count_model.vocabulary_\n",
    "    vocab2 = {y:x for x,y in vocab.items()}\n",
    "    return create_network(Xc, vocab2, graph_settings)\n",
    "    #ALTERNATIVE WAY TO COMPUTE MATRIX\n",
    "    # occurrences = OrderedDict((name, OrderedDict((name, 0) for name in termSplit)) for name in termSplit)\n",
    "    # # Find the co-occurrences:\n",
    "    # for l in document:\n",
    "    #     for i in range(len(l)):\n",
    "    #         for item in l[:i] + l[i + 1:]:\n",
    "    #             occurrences[l[i]][item] += 1\n",
    "    # # Print the matrix:\n",
    "    # print(' ', ' '.join(occurrences.keys()))\n",
    "    # for name, values in occurrences.items():\n",
    "    #     print(name, ' '.join(str(i) for i in values.values()))\n",
    "    \n",
    "def create_network(matrix, vocab, graph_settings):\n",
    "    G = nx.from_scipy_sparse_matrix(matrix)\n",
    "    H = nx.relabel_nodes(G, vocab)\n",
    "    data = json_graph.node_link_data(H)\n",
    "    T = json_graph.node_link_graph(data)\n",
    "    ns = list(T.nodes)\n",
    "    es = list(T.edges)\n",
    "    final_nodes = []\n",
    "    for n in G.nodes:\n",
    "        nod = {'name': ns[n], 'id':n}\n",
    "        final_nodes.append(nod)\n",
    "\n",
    "    N = len(T.nodes)\n",
    "    counts = np.zeros((N, N))\n",
    "    for e in G.edges(data=True):\n",
    "        source, target, w = e\n",
    "        counts[[source], [target]] = w['weight']\n",
    "        counts[[target], [source]] = w['weight']\n",
    "    print(len(final_nodes))\n",
    "    return draw_graph(counts, final_nodes, graph_settings, ns)\n",
    "        \n",
    "def draw_graph(counts, nodes, graph_settings, list_nodes):\n",
    "    xname = []\n",
    "    yname = []\n",
    "    color = []\n",
    "    alpha = []\n",
    "#     colormap = [\"#444444\", \"#a6cee3\", \"#1f78b4\", \"#b2df8a\", \"#33a02c\", \"#fb9a99\",\"#e31a1c\", \"#fdbf6f\", \"#ff7f00\", \"#cab2d6\", \"#6a3d9a\"]\n",
    "    for i, node1 in enumerate(nodes):\n",
    "        for j, node2 in enumerate(nodes):\n",
    "            xname.append(node1['name'])\n",
    "            yname.append(node2['name'])\n",
    "\n",
    "            alpha.append(min(counts[i,j]/4.0, 0.9) + 0.1)\n",
    "\n",
    "    for i in range(len(xname)):\n",
    "        al = alpha[i]\n",
    "        if  al == 0.35:\n",
    "            color.append('#ce93d8')\n",
    "        elif al == 0.6:\n",
    "            color.append('#ba68c8')\n",
    "        elif al == 0.85:\n",
    "            color.append('#9c27b0')\n",
    "        elif al == 1.0:\n",
    "            color.append('#7b1fa2')\n",
    "        else:\n",
    "            color.append('lightgrey')\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(\n",
    "        xname=xname,\n",
    "        yname=yname,\n",
    "        colors=color,\n",
    "        alphas=alpha,\n",
    "        count=counts.flatten(),\n",
    "    ))\n",
    "\n",
    "    p = figure(title=graph_settings['title'],\n",
    "               x_axis_location=\"above\", tools=\"hover,save\",\n",
    "               x_range=list(reversed(list_nodes)), y_range=list_nodes)\n",
    "\n",
    "    p.plot_width = graph_settings['width']\n",
    "    p.plot_height = graph_settings['height']\n",
    "    p.grid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"5pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = np.pi/3\n",
    "\n",
    "    p.rect('xname', 'yname', 0.9, 0.9, source=source,\n",
    "           color='colors', alpha='alphas', line_color=None,\n",
    "           hover_line_color='black', hover_color='colors')\n",
    "\n",
    "    p.select_one(HoverTool).tooltips = [\n",
    "        ('names', '@yname, @xname'),\n",
    "        ('count', '@count'),\n",
    "    ]\n",
    "\n",
    "   \n",
    "\n",
    "    return p # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "48\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "output_file(\"OCR_coocurence_matrix.html\", title='Co-Occurence Test')\n",
    "order_settings = {'title': 'Ordered_Text_AO', 'height': 600, 'width': 600}\n",
    "order_types = ['GPE']\n",
    "order_p = process_text(order_df, order_types, order_settings)\n",
    "\n",
    "full_settings = {'title': 'Full_Text_AO', 'height': 600, 'width': 600}\n",
    "full_types = ['GPE']\n",
    "full_p = process_text(full_df, full_types, full_settings)\n",
    "\n",
    "split_settings = {'title': 'Split_Text_AO', 'height': 600, 'width': 600}\n",
    "split_types = ['GPE']\n",
    "split_p = process_text(split_df, split_types, split_settings)\n",
    "\n",
    "show(row(order_p, split_p, full_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    if not text:\n",
    "#       print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def process_model_text(df):\n",
    "\n",
    "    final_doc = []\n",
    "    for index, row in df.iterrows():\n",
    "        raw_text = row['google_vision_text']\n",
    "        tokens = custom_tokenize(raw_text)\n",
    "        doc = []\n",
    "        for t in tokens:\n",
    "            \n",
    "            if t in string.punctuation:\n",
    "                pass\n",
    "            elif t in stopwords.words('english'):\n",
    "                pass\n",
    "            else:\n",
    "                doc.append(t.lower())\n",
    "        final_doc.append(doc)\n",
    "    create_models(final_doc)\n",
    "\n",
    "def create_models(texts):\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lda = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, passes=10)\n",
    "    print(lda.show_topics())\n",
    "    return pyLDAvis.gensim.prepare(lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11, '0.012*\"the\" + 0.009*\"one\" + 0.005*\"world\" + 0.005*\"workers\" + 0.004*\"kennedy\" + 0.004*\"egypt\" + 0.004*\"it\" + 0.003*\"american\" + 0.003*\"mrs.\" + 0.003*\"time\"'), (18, '0.016*\"the\" + 0.009*\"arab\" + 0.008*\"workers\" + 0.006*\"one\" + 0.005*\"petroleum\" + 0.005*\"dr.\" + 0.005*\"\\'s\" + 0.004*\"beethoven\" + 0.004*\"cairo\" + 0.004*\"federation\"'), (5, '0.024*\"the\" + 0.020*\"funds\" + 0.016*\"mr.\" + 0.013*\"hammer\" + 0.013*\"chairman\" + 0.011*\"uia\" + 0.010*\"zionist\" + 0.009*\"agency\" + 0.008*\"shekel\" + 0.007*\"received\"'), (13, '0.016*\"the\" + 0.011*\"workers\" + 0.010*\"it\" + 0.007*\"federation\" + 0.006*\"arab\" + 0.006*\"\\'s\" + 0.006*\"would\" + 0.006*\"president\" + 0.005*\"poetry\" + 0.005*\"petroleum\"'), (17, '0.008*\"poetry\" + 0.007*\"art\" + 0.007*\"professor\" + 0.006*\"new\" + 0.005*\"decamps\" + 0.005*\"ziegler\" + 0.004*\"the\" + 0.004*\"though\" + 0.004*\"work\" + 0.004*\"way\"'), (0, '0.020*\"the\" + 0.012*\"arab\" + 0.008*\"\\'s\" + 0.008*\"kennedy\" + 0.007*\"israel\" + 0.006*\"president\" + 0.005*\"made\" + 0.005*\"man\" + 0.005*\"party\" + 0.005*\"army\"'), (10, '0.008*\"the\" + 0.008*\"beethoven\" + 0.006*\"\\'s\" + 0.006*\"bonn\" + 0.006*\"statue\" + 0.005*\"it\" + 0.005*\"city\" + 0.005*\"every\" + 0.005*\"coptic\" + 0.005*\"art\"'), (6, '0.013*\"arab\" + 0.010*\"workers\" + 0.009*\"petroleum\" + 0.008*\"general\" + 0.007*\"the\" + 0.005*\"he\" + 0.005*\"in\" + 0.005*\"time\" + 0.004*\"labour\" + 0.004*\"federation\"'), (16, '0.027*\"in\" + 0.025*\"g.m.t\" + 0.012*\"kc\" + 0.012*\"metres\" + 0.009*\"the\" + 0.006*\"huxley\" + 0.006*\"--\" + 0.005*\"piano\" + 0.005*\"to\" + 0.005*\"arabic\"'), (8, '0.025*\"africa\" + 0.024*\"south\" + 0.014*\"portugal\" + 0.013*\"the\" + 0.012*\"said\" + 0.011*\"\\'s\" + 0.010*\"west\" + 0.009*\"african\" + 0.007*\"would\" + 0.006*\"u.n.\"')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zgl3n/.local/share/virtualenvs/data-analysis-GZuBTKDw/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_data = process_model_text(order_df)\n",
    "vis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    if not text:\n",
    "#       print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def process_text(df):\n",
    "    doc = []\n",
    "    final_doc = []\n",
    "    for index, row in df.iterrows():\n",
    "        raw_text = row['google_vision_text']\n",
    "        tokens = custom_tokenize(raw_text)\n",
    "        page_terms = ''\n",
    "        for t in tokens:\n",
    "            if t in string.punctuation:\n",
    "                pass\n",
    "            elif t in stopwords.words('english'):\n",
    "                pass\n",
    "            else:\n",
    "                page_terms += t.lower() + ' '\n",
    "        doc.append(page_terms)\n",
    "\n",
    "    for sent in doc:\n",
    "        sent_terms = ''\n",
    "        spacy_text = nlp(sent)\n",
    "        for ent in spacy_text.ents:\n",
    "            if ent.label_ in types:\n",
    "                sent_terms += ent.text + ' '\n",
    "        final_doc.append(sent_terms)\n",
    "    return create_matrix(final_doc, graph_settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
